
---

## ğŸ“± Jumia Phone Tracker â€” Web Scraping Project

A data extraction tool built with **Scrapy** to crawl and collect mobile phone listings from [Jumia Nigeria](https://www.jumia.com.ng/mobile-phones/). The spider extracts structured product data and stores it into **MongoDB** for easy analysis and tracking, and **automatically exports it to a connected Google Sheet**.

---

### ğŸ§° Tech Stack

* **Scrapy** (Core framework)
* **MongoDB** (Data storage)
* **Google Sheets API** (Cloud sync)
* **Custom Middleware**
  * `RotateHeadersMiddleware` â€“ Randomly rotates user agents and languages to avoid detection
* **AutoThrottle** â€“ Manages crawl speed dynamically based on site response time

---

### ğŸ“¦ Features

* âœ… Crawls up to 4 pages of mobile phone listings (configurable)
* âœ… Designed to **scale to more pages** â€” page limit set to 4 intentionally to:
  * Avoid hammering Jumiaâ€™s servers
  * Stay within ethical scraping limits
  * Keep test runs short during development
* âœ… Extracts:
  * Product title
  * Price
  * Rating
  * Stock availability
  * Product URL
* âœ… Skips products with missing data gracefully
* âœ… Saves data directly to MongoDB in a collection named after the spider
* âœ… Automatically exports each item to **Google Sheets**
* âœ… Logs pagination progress and crawl status

---

### ğŸ“¤ Google Sheets Export (Cloud Sync)

Each scraped item is **immediately synced to a Google Sheet** using the [Google Sheets API](https://developers.google.com/sheets/api).

Benefits:
* Access your scraped data from anywhere in real-time
* Easily collaborate, filter, and analyze data in the cloud
* No manual copy-pasting â€” everything is automated!

---

### ğŸ“¸ Google Sheet Output Snapshot

![Google Sheet Snapshot](assets/google_sheet_output.PNG)

---

### ğŸ” Setup

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create OAuth 2.0 credentials for "Desktop App"
3. Download the `credentials.json` file and place it in your project root
4. The first time you run the spider, a browser window will open to authorize access
5. A `token.json` will be saved and reused for future runs

---

### ğŸ“ Folder Structure

```shell
jumia_phone_tracker/
â”œâ”€â”€ spiders/
â”‚   â””â”€â”€ phone_spider.py       # Main spider logic
â”œâ”€â”€ middlewares.py            # Downloader & spider middleware, header rotation
â”œâ”€â”€ pipelines.py              # MongoDB + Google Sheets storage logic
â”œâ”€â”€ settings.py               # All project settings
â”œâ”€â”€ credentials.json          # Google Sheets API credentials
â”œâ”€â”€ token.json                # Saved token after first auth (auto-generated)
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ google_sheet_output.png  # Screenshot of Google Sheets output
````

---

### ğŸš€ How to Run the Spider

1. **Install requirements**

   ```shell
   pip install scrapy pymongo google-auth google-auth-oauthlib google-api-python-client
   ```

2. **Start MongoDB** (locally or with Atlas)

3. **Run the spider**

   ```shell
   scrapy crawl phone_spider
   ```

4. **Check your MongoDB and Google Sheet**
   Your data will be stored both:

   * Locally in MongoDB (`jumia_phone_tracker.phone_spider` collection)
   * In your configured **Google Sheet**

---

### âœ… Sample Output

```json
{
  "titles": "Samsung Galaxy A15",
  "prices": "â‚¦123,000",
  "ratings": "4.3 out of 5",
  "availability": "Only 3 units left",
  "links": "https://www.jumia.com.ng/samsung-a15-4gb-128gb..."
}
```

---

### ğŸ“Œ Notes

* The page limit is **configurable** and can easily be increased or removed for broader crawls.
* Keeping it to 4 pages during development was a conscious choice to:

  * Avoid triggering anti-bot systems
  * Respect the server load
  * Speed up testing and iteration
* This approach demonstrates **scalability** and **scraper etiquette**.
* Respects ethical scraping principles by limiting crawl depth and using throttling.
* Does **not** obey `robots.txt` (for educational purposes only).

---

### ğŸ’¼ Portfolio Use

This project is perfect for showcasing:

* Custom Scrapy middleware development
* Integration with MongoDB and Google Sheets
* Smart pagination and data cleaning
* Scalable scraping architecture
* API authorization and cloud syncing
