
## ğŸ“± Jumia Phone Tracker â€” Web Scraping Project

A data extraction tool built with **Scrapy** to crawl and collect mobile phone listings from [Jumia Nigeria](https://www.jumia.com.ng/mobile-phones/). The spider extracts structured product data and stores it into **MongoDB** for easy analysis and tracking.

---

### ğŸ§° Tech Stack

* **Scrapy** (Core framework)
* **MongoDB** (Data storage)
* **Custom Middleware**

  * `RotateHeadersMiddleware` â€“ Randomly rotates user agents and languages to avoid detection
* **AutoThrottle** â€“ Manages crawl speed dynamically based on site response time

---

### ğŸ“¦ Features

* âœ… Crawls up to 4 pages of mobile phone listings (configurable)
* âœ… Designed to **scale to more pages** â€” page limit set to 4 intentionally to:

  * Avoid hammering Jumiaâ€™s servers
  * Stay within ethical scraping limits
  * Keep test runs short during development
* âœ… Extracts:

  * Product title
  * Price
  * Rating
  * Stock availability
  * Product URL
* âœ… Skips products with missing data gracefully
* âœ… Saves data directly to MongoDB in a collection named after the spider
* âœ… Logs pagination progress and crawl status

---

### ğŸ“ Folder Structure

```shell
jumia_phone_tracker/
â”œâ”€â”€ spiders/
â”‚   â””â”€â”€ phone_spider.py       # Main spider logic
â”œâ”€â”€ middlewares.py            # Downloader & spider middleware, header rotation
â”œâ”€â”€ pipelines.py              # MongoDB storage logic
â”œâ”€â”€ settings.py               # All project settings
```

---

### ğŸš€ How to Run the Spider

1. **Install requirements**

   ```shell
   pip install scrapy pymongo
   ```

2. **Start MongoDB** (locally or with Atlas)

3. **Run the spider**

   ```shell
   scrapy crawl phone_spider
   ```

4. **Check MongoDB**
   Your data will be in the `jumia_phone_tracker.phone_spider` collection.

---

### âœ… Sample Output

```json
{
  "titles": "Samsung Galaxy A15",
  "prices": "â‚¦123,000",
  "ratings": "4.3 out of 5",
  "availability": "Only 3 units left",
  "links": "https://www.jumia.com.ng/samsung-a15-4gb-128gb..."
}
```

---

### ğŸ“Œ Notes

* The page limit is **configurable** and can easily be increased or removed for broader crawls.
* Keeping it to 4 pages during development was a conscious choice to:

  * Avoid triggering anti-bot systems
  * Respect the server load
  * Speed up testing and iteration
* This approach demonstrates **scalability** and **scraper etiquette**.
* Respects ethical scraping principles by limiting crawl depth and using throttling.
* Does **not** obey `robots.txt` (for educational purposes only).

---

### ğŸ’¼ Portfolio Use

This project is perfect for showcasing:

* Custom Scrapy middleware development
* Integration with MongoDB
* Smart pagination and data cleaning
* Scalable scraping architecture
